{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../../datasets/main_data/bank-additional-full.csv'\n",
    "full_bank = pd.read_csv(path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module Data.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.py\n",
    "# %%writefile ../scripts/data.py\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "def load_data(path=\"\", sep=\",\", cols_to_drop=[]):\n",
    "    data = pd.read_csv(path, sep)\n",
    "    for col in cols_to_drop:\n",
    "        data.drop(col, axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def check_outliers(data, show_plot=False, save_img=os.getcwd()+'/outliers.png'):\n",
    "    \n",
    "    \"\"\"\n",
    "    This functions checks for columns with outlers using the IQR method\n",
    "    \n",
    "    It accespts as argmuent a dataset. \n",
    "    show_plot can be set to True to output pairplots of outlier columns    \n",
    "    \"\"\"\n",
    "    \n",
    "    outliers = [] \n",
    "    Q1 = data.quantile(0.25)  \n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    num_data = data.select_dtypes(include='number')\n",
    "    result = dict ((((num_data < (Q1 - 1.5 * IQR)) | (num_data > (Q3 + 1.5 * IQR)))==True).any())\n",
    "    #data[(data[col] >= high)|(data[col] <= low)].index\n",
    "    index = data[(num_data < Q1 - 1.5 * IQR) | (num_data > Q3 + 1.5 * IQR)].index\n",
    "    for k,v in result.items():\n",
    "        if v == True:  \n",
    "            outliers.append(k)\n",
    "    if show_plot:\n",
    "        pair_plot = sns.pairplot(data[outliers]);\n",
    "        print(f'{result},\\n\\n Visualization of outlier columns')\n",
    "        plt.savefig(fname=save_img, format='png')\n",
    "        return pair_plot\n",
    "    else:\n",
    "        return data.loc[index, outliers]\n",
    "    \n",
    "    \n",
    "\n",
    "def treat_outliers(data, type='median_replace'):\n",
    "    \n",
    "    \"\"\"\n",
    "    This treat outliers using any ofthses 3 methods as specified by user\n",
    "    \n",
    "        1. median_replace -  median replacement\n",
    "        \n",
    "        2. quant_floor - quantile flooring\n",
    "        \n",
    "        3. trim - trimming \n",
    "        \n",
    "        4. log_transform - log transformations\n",
    "    \n",
    "    The methods are some of the commont statistical methods in treating outler\n",
    "    columns\n",
    "    \n",
    "    By default treatment type is set to median replacement\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if type == \"median_replace\":\n",
    "        \n",
    "        for col in data.columns.tolist():\n",
    "            if is_numeric_dtype(data[col]):\n",
    "                median = (data[col].quantile(0.50))\n",
    "                print(median)\n",
    "                q1 = data[col].quantile(0.25)\n",
    "                q3 = data[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                high = int(q3 + 1.5 * iqr) \n",
    "                low = int(q1 - 1.5 * iqr)\n",
    "                print(high, low, iqr)\n",
    "                print(col)\n",
    "                data[col] = np.where(data[col] > high, median, data[col])\n",
    "                data[col] = np.where(data[col] > high, median, data[col])        \n",
    "    \n",
    "    if type == \"quant_floor\":\n",
    "        \n",
    "        for col in data.columns.tolist():\n",
    "            if is_numeric_dtype(data[col]):\n",
    "                q_10 = data[col].quantile(0.5)\n",
    "                q_90 = data[col].quantile(0.95)\n",
    "                data[col] =  data[col] = np.where(data[col] < q_10, q_10 , data[col])\n",
    "                data[col] =  data[col] = np.where(data[col] > q_90, q_90 , data[col])\n",
    "            \n",
    "    if type == \"trim\":\n",
    "        \n",
    "        for col in data.columns.tolist():\n",
    "            low = .05\n",
    "            high = .95\n",
    "            quant_df = data.quantile([low, high])\n",
    "            for name in list(data.columns):\n",
    "                if is_numeric_dtype(data[name]):\n",
    "                    data = data[(data[name] >= quant_df.loc[low, name]) \n",
    "                        & (data[name] <= quant_df.loc[high, name])]\n",
    "            \n",
    "    if type == \"log_transform\":  \n",
    "        for col in data.columns.tolist():\n",
    "            if is_numeric_dtype(data[col]):\n",
    "                data[col] = data[col].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "                \n",
    "    if type == \"isf\": \n",
    "        iso = IsolationForest(contamination=0.1)\n",
    "        yhat = iso.fit_predict(data.select_dtypes(exclude='object'))\n",
    "        #select all rows that are not outliers\n",
    "        mask = yhat != -1 \n",
    "        data = data[mask]\n",
    "        \n",
    "\n",
    "    return data \n",
    "\n",
    "\n",
    "def scale_data(data,scaler=RobustScaler()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Specify scaler type, scaler type must have fit_transform as a method\n",
    "    \n",
    "    \"\"\"\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module Plot.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot.py \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import os\n",
    "\n",
    "def plot_univariate (data, x=None, y=None, color='r',save=False,\n",
    "                title='New Chart', chart_type='hist', xlabel='', ylabel='',\n",
    "                    save_to=os.getcwd(), log_normalise=False):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Make a univariate plot of any of these selcted types:\n",
    "    \n",
    "    1. bar - barchart\n",
    "    \n",
    "    2. hist - Histogram\n",
    "    \n",
    "    3. pie - Piechart\n",
    "    \n",
    "    4. count - Countplot\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.subplots(figsize=(10,7))\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel(ylabel, fontsize=15)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    \n",
    "    if chart_type == 'hist':\n",
    "        if log_normalise:\n",
    "            data = np.log(data)\n",
    "        plot = sns.distplot(a=data, color=color)\n",
    "        if save:\n",
    "            plt.savefig(fname=save_to+f'/{title}.png', format='png')\n",
    "        \n",
    "    return plot\n",
    "\n",
    "def plot_bivariate(data, x=None, y=None, hue=None, \n",
    "                  color='r',save=False,\n",
    "                title='New Chart', chart_type='hist',\n",
    "                   xlabel='', ylabel='',\n",
    "                    save_to=os.getcwd(), img_name = \" \", \n",
    "                   palette={'use':False, \"size\":1}, log_normalise=False,\n",
    "                  kind_joint_plot = 'scatter', kind_pair_plot=\"scatter\", figsize=(10,7)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make a bivariate plot of any of the selcted types:\n",
    "    \n",
    "    1. bar - barchart\n",
    "    \n",
    "    2. scatter  - scatter plot\n",
    "    \n",
    "    3. cat  - catplot\n",
    "    \n",
    "    4. count - countplot\n",
    "    \n",
    "    5 joint - jointplot \n",
    "    \n",
    "    6  pair - pairplot\n",
    "    \n",
    "    7  corr - corr_plot\n",
    "    \n",
    "    When calling joint_plot:\n",
    "        \n",
    "        kind_joint_plot is default to `scatter`\n",
    "        other types include \"reg\", \"reside\", \"kde\", \"hex\"\n",
    "        \n",
    "    When calling pair_plot:\n",
    "        \n",
    "        kind_pair_plot is default to `scatter`\n",
    "        other types include 'reg'\n",
    "    \"\"\"\n",
    "    def plt_tweaks():\n",
    "        plt.subplots(figsize= figsize)\n",
    "        plt.title(title, fontsize=18)\n",
    "        plt.xlabel(xlabel, fontsize=15)\n",
    "        plt.ylabel(ylabel, fontsize=15)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "    \n",
    "    \n",
    "    # define helper functions\n",
    "    \n",
    "    def use_palette():\n",
    "        palettes = []\n",
    "#        palette_to_use=[]\n",
    "        if palette['use'] == True:\n",
    "            palette_to_use = [palettes[i] for i in range(palette['size'])]\n",
    "            \n",
    "            return palette_to_use\n",
    "\n",
    "    def log_norm():\n",
    "        if log_normalise and y != None:\n",
    "            y = np.log(y)\n",
    "        elif log_normalise and y == None:\n",
    "            data = np.log(data)\n",
    "            \n",
    "    def save_image():\n",
    "        if save:\n",
    "            if img_name != \" \":\n",
    "                plt.savefig(fname=save_to+\"/\"+img_name+'.png', format='png')\n",
    "            else:\n",
    "                plt.savefig(fname=save_to+f'/{title}.png', format='png')\n",
    "                \n",
    "        \n",
    "    # make plots\n",
    "    \n",
    "    if chart_type == \"joint\":\n",
    "        log_norm()\n",
    "        plot = sns.jointplot(x=x, y=y, data=data,\n",
    "                            height=6, ratio=5, space=0.2, kind=kind_joint_plot)\n",
    "        \n",
    "        save_image()\n",
    "        \n",
    "    if chart_type == \"pair\":\n",
    "       # try:\n",
    "        log_norm()\n",
    "        if palette['use'] == True:\n",
    "            palette_to_use = use_palette()\n",
    "            plot = sns.pairplot(data, palette=palette_to_use, \n",
    "                            kind= kind_pair_plot,height=3, aspect=1, hue=hue)\n",
    "        else:\n",
    "             plot = sns.pairplot(data, \n",
    "                            kind= kind_pair_plot,height=2.5, aspect=1, hue=hue, )\n",
    "        save_image()\n",
    "        \n",
    "    if chart_type  == \"corr\":\n",
    "        plt_tweaks()\n",
    "        corr_data = data.corr()\n",
    "        corr_plot = sns.heatmap(corr_data,annot=True, fmt='.2g', center=0) \n",
    "\n",
    "def plot_univariate (data, x=None, y=None, color='r',save=False,\n",
    "                title='New Chart', chart_type='hist', xlabel='', ylabel='',\n",
    "                    save_to=os.getcwd(), log_normalise=False):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Make a univariate plot of any of these selcted types:\n",
    "    \n",
    "    1. bar - barchart\n",
    "    \n",
    "    2. hist - Histogram\n",
    "    \n",
    "    3. pie - Piechart \n",
    "    \n",
    "    4. count - Countplot\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.subplots(figsize=(10,7))\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel(ylabel, fontsize=15)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    \n",
    "    if chart_type == 'hist':\n",
    "        if log_normalise:\n",
    "            data = np.log(data)\n",
    "        plot = sns.distplot(a=data, color=color)\n",
    "        if save:\n",
    "            plt.savefig(fname=save_to+f'/{title}.png', format='png')\n",
    "        \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module Model.py Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE, _random_over_sampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.pipeline import Pipeline as ImbPipe\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "\n",
    "def plot_pca_components(data):\n",
    "    pca = PCA().fit(data)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance');\n",
    "    \n",
    "def check_imbalance(data,label='', x=0.7, y=30000):\n",
    "    plt.subplots(figsize=(10,8))\n",
    "    data[label].value_counts().plot(kind='bar')\n",
    "    text = f'Class Imbalance Count:\\n\\n{data[label].value_counts().to_dict()}'\n",
    "    plt.text(x=x, y=y, s = text ,  fontsize=15)\n",
    "    \n",
    "def encode (data):\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "    to_encode = data.select_dtypes(exclude='number')\n",
    "    if data.shape[1] > 1:\n",
    "        #ohe = MultiLabelBinarizer()\n",
    "        data.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "        features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "        data = data.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "        #print(ohe.classes_) \n",
    "    else:\n",
    "        data = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "        print(ohe.categories_) \n",
    "    return data \n",
    "\n",
    " \n",
    "\n",
    "def x_y_split(data, x=None, y=None, type_=\"single\", test_size=.10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Single type divides into just x and y\n",
    "    Double type divides into train and test for each of x and y\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = data.drop(columns=y, axis=1), data[y]\n",
    "    \n",
    "    if type_ == \"single\":\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    if type == \"double\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                               test_size=test_size, random_state=123)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "def model_pipeline(X_train=None, y_train=None, X_test=None, pca=PCA(), \n",
    "                   cv=StratifiedKFold(), imb_sample=SMOTE(random_state=123),\n",
    "                  model=LogisticRegressionCV()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a model for an imbalanced class using the specified estimator\n",
    "    The training is done in K-folds or its nuances as specified folds \n",
    "    applying the specified sampling strategy\n",
    "    \"\"\"\n",
    "    \n",
    "    model = ImbPipe([('imb_sample', imb_sample), ('pca', pca), ('model', model)])\n",
    "    model.fit(X_train, y_train) \n",
    "    y_hat = model.predict(X_test) \n",
    "    return model, y_hat\n",
    "    \n",
    "    \n",
    "def gridSearch(model,hyper_params={},cv=StratifiedKFold(), x_train=None, y_train=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs GridSeach of the best hyperparmaters for the passed model\n",
    "    \"\"\"\n",
    "    \n",
    "    search = GridSearchCV(model=model, param_grid = hyper_params, n_jobs=-1, cv=cv)\n",
    "    search.fit(X=x_train, y=y_train)\n",
    "    print(\"Best parameter (CV score=%0.3f):\\n\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "    print(search.score) \n",
    "    return search\n",
    "\n",
    "\n",
    "def plot_grid_search(search_obj, pca_obj, X_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints the best (optimised) hyperparmatersfor the grid search object\n",
    "    and plots the optimised pca components\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Best parameter (CV score=%0.3f):\\n\" % search.best_score_)\n",
    "    print(\"Best Params:\",search.best_params_)\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(8, 8))\n",
    "    ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "             pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "    ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "    ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "                linestyle=':', label='n_components chosen')\n",
    "    ax0.legend(prop=dict(size=12))\n",
    "\n",
    "    # For each number of components, find the best classifier results\n",
    "    results = pd.DataFrame(search.cv_results_)\n",
    "    components_col = 'param_pca__n_components'\n",
    "    best_clfs = results.groupby(components_col).apply(\n",
    "        lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "    best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "                   legend=False, ax=ax1)\n",
    "    ax1.set_ylabel('Classification accuracy (val)')\n",
    "    ax1.set_xlabel('n_components')\n",
    "\n",
    "    plt.xlim(-1, 70)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "\n",
    "class metrics ():\n",
    "    \n",
    "    def __init__(self, y_test, y_hat):\n",
    "        pass\n",
    "        self.y_test = y_test\n",
    "        self.y_hat =  y_hat\n",
    "        \n",
    "    \n",
    "    def class_report(self):\n",
    "        \n",
    "        full_report = classification_report(self.y_test, self.y_hat)\n",
    "        \n",
    "        print(full_report)\n",
    "        \n",
    "    def conf_matrix(self):\n",
    "        \n",
    "        conf_matrix = confusion_matrix(self.y_test, self.y_hat)\n",
    "        \n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Actual_+ve', 'Actual_-ve'],\n",
    "                               index=['predicted_+ve', 'predicted_-ve'])\n",
    "        \n",
    "        return conf_matrix_df\n",
    "    \n",
    "    def accuracy_score(self):\n",
    "        return  accuracy_score(self.y_test, self.y_hat)\n",
    "    \n",
    "    def classification_error(self):\n",
    "        \n",
    "        return 1 - accuracy_score() \n",
    "        \n",
    "    def specif_sensitiv(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Sensitivity: When the actual value is positive, how often is the prediction correct?\n",
    "        \n",
    "        Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "        \"\"\"\n",
    "        \n",
    "        conf_matrix = confusion_matrix(self.y_test, self.y_hat)\n",
    "        \n",
    "        TP = conf_matrix[1, 1]\n",
    "        TN = conf_matrix[0, 0]\n",
    "        FP = conf_matrix[0, 1]\n",
    "        FN = conf_matrix[1, 0]\n",
    "        \n",
    "        sensitivity = TP / float(FN + TP)\n",
    "        specificity = TN / (TN + FP)\n",
    "        \n",
    "        sensitiv_specific_table = pd.DataFrame([[sensitivity, specificity]],\n",
    "                                               columns=['sensitivity', 'specificity'])\n",
    "        \n",
    "        return sensitiv_specific_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "# %%writefile ../scripts/data.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from pandas.api.types import is_numeric_dtype \n",
    "\n",
    "from plot import plot_univariate, plot_bivariate\n",
    "from data import check_outliers, treat_outliers, scale_data, load_data\n",
    "from model import check_imbalance, plot_pca_components, encode, x_y_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE, _random_over_sampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_classif, from_model, SelectKBest,chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest # outlier detection and re,oval\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from imblearn.pipeline import Pipeline as ImbPipe\n",
    "import joblib\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_termsheet_pred_model",
   "language": "python",
   "name": "venv_termsheet_pred_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
