{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../../datasets/main_data/bank-additional-full.csv'\n",
    "full_bank = pd.read_csv(path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module data.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.py\n",
    "#%%writefile ../scripts/project_package/data_package/data.py\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "import pandas\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.base import *\n",
    "\n",
    "\n",
    "class WrangleData():\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return \"Used to prepare data for wrangling\"\n",
    "    \n",
    "    def  __init__(self):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    \n",
    "    def load_data(self, path=\"\", sep=\",\", cols_to_drop=[]):\n",
    "                \n",
    "        self.path = path\n",
    "        self.cols_to_drop = cols_to_drop \n",
    "        self.sep = sep  \n",
    "            \n",
    "        try :\n",
    "            self.data = pd.read_csv(path, sep)\n",
    "            \n",
    "            if len(self.cols_to_drop) > 0:\n",
    "                for col in self.cols_to_drop:\n",
    "                    self.data.drop(col, axis=1, inplace=True)\n",
    "                        \n",
    "            self._fit() \n",
    "            \n",
    "            return self.data \n",
    "        \n",
    "        except:\n",
    "            \n",
    "            \"No data path was passed upon call of method load_data\"\n",
    "    \n",
    "    def _fit(self):\n",
    "        \n",
    "        try:\n",
    "            assert(type(self.data) is pandas.core.frame.DataFrame), \"data must be of type pandas.DataFrame\"\n",
    "            \n",
    "            print(\"You are now fit to use this object for wrangling\")\n",
    "                    \n",
    "        except AttributeError:\n",
    "            \n",
    "            print(\"Hey Buddy you need to load a data first !!! \")\n",
    "        \n",
    "\n",
    "    def get_data(self):\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "    def check_outliers(self, show_plot=False, save_img=os.getcwd()+'/outliers.png'):\n",
    " \n",
    "        \"\"\"\n",
    "        This functions checks for columns with outlers using the IQR method\n",
    "\n",
    "        It accespts as argmuent a dataset. \n",
    "        show_plot can be set to True to output pairplots of outlier columns    \n",
    "        \"\"\"\n",
    "\n",
    "        self.outliers = [] \n",
    "        Q1 = self.data.quantile(0.25)  \n",
    "        Q3 = self.data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        num_data = self.data.select_dtypes(include='number')\n",
    "        result = dict ((((num_data < (Q1 - 1.5 * IQR)) | (num_data > (Q3 + 1.5 * IQR)))==True).any())\n",
    "        #data[(data[col] >= high)|(data[col] <= low)].index\n",
    "        index = self.data[(num_data < Q1 - 1.5 * IQR) | (num_data > Q3 + 1.5 * IQR)].index\n",
    "        for k,v in result.items():\n",
    "            if v == True:  \n",
    "                self.outliers.append(k)\n",
    "        if show_plot:\n",
    "            self.outlier_pair_plot = sns.pairplot(self.data[self.outliers]);\n",
    "            print(f'{result},\\n\\n Visualization of outlier columns')\n",
    "            plt.savefig(fname=save_img, format='png')\n",
    "            return  self.outlier_pair_plot\n",
    "        else:\n",
    "            return self.data.loc[index, self.outliers] \n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "    def treat_outliers(self, type_='median_replace'):\n",
    "            \n",
    "        \"\"\"\n",
    "        This treat outliers using any ofthses 3 methods as specified by user\n",
    "\n",
    "            1. median_replace -  median replacement\n",
    "\n",
    "            2. quant_floor - quantile flooring\n",
    "\n",
    "            3. trim - trimming \n",
    "\n",
    "            4. log_transform - log transformations\n",
    "            \n",
    "            5. isf    -       IsolationForest (also like trimming)\n",
    "\n",
    "        The methods are some of the commont statistical methods in treating outler\n",
    "        columns\n",
    "\n",
    "        By default treatment type is set to median replacement\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if type_ == \"median_replace\":\n",
    "\n",
    "            for col in self.data.columns.tolist():\n",
    "                if is_numeric_dtype(self.data[col]):\n",
    "                    median = (self.data[col].quantile(0.50))\n",
    "                    q1 = self.data[col].quantile(0.25)\n",
    "                    q3 = self.data[col].quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    high = int(q3 + 1.5 * iqr) \n",
    "                    low = int(q1 - 1.5 * iqr)\n",
    "                    self.data[col] = np.where(self.data[col] > high, median, self.data[col])\n",
    "                    self.data[col] = np.where(self.data[col] > high, median, self.data[col])        \n",
    "\n",
    "        if type_ == \"quant_floor\":\n",
    "\n",
    "            for col in self.data.columns.tolist():\n",
    "                if is_numeric_dtype(data[col]):\n",
    "                    q_10 = self.data[col].quantile(0.5)\n",
    "                    q_90 = self.data[col].quantile(0.95)\n",
    "                    self.data[col] =  self.data[col] = np.where(self.data[col] < q_10, q_10 , self.data[col])\n",
    "                    self.data[col] =  self.data[col] = np.where(self.data[col] > q_90, q_90 , self.data[col])\n",
    "\n",
    "        if type_ == \"trim\": \n",
    "\n",
    "            for col in self.data.columns.tolist():\n",
    "                low = .05\n",
    "                high = .95\n",
    "                quant_df = self.data.quantile([low, high])\n",
    "                for name in list(self.data.columns):\n",
    "                    if is_numeric_dtype(self.data[name]):\n",
    "                        self.data = self.data[(self.data[name] >= quant_df.loc[low, name]) \n",
    "                            & (self.data[name] <= quant_df.loc[high, name])]\n",
    "\n",
    "        if type_ == \"log_transform\":  \n",
    "            for col in self.data.columns.tolist():\n",
    "                if is_numeric_dtype(self.data[col]):\n",
    "                    self.data[col] = self.data[col].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "\n",
    "        if type_ == \"isf\":\n",
    "            iso = IsolationForest(contamination=0.1)\n",
    "            yhat = iso.fit_predict(self.data.select_dtypes(exclude='object'))\n",
    "            #select all rows that are not outliers\n",
    "            mask = yhat != -1 \n",
    "            self.data = self.data[mask]\n",
    "\n",
    "\n",
    "        return self.data  \n",
    "    \n",
    "    \n",
    "    def map_col_values(self, col_name=\"\", values_dict={}):\n",
    "        \n",
    "        \"\"\"\n",
    "        replace values in a series (values_dict.keys) with specified values from (values_dict.values)\n",
    "        \"\"\"\n",
    "\n",
    "        self.data[col_name] = self.data[col_name].map(values_dict)\n",
    "\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    def split_data_single(self, target_cols=[]):\n",
    "            \n",
    "        self.split1 = self.data.drop(columns=target_cols, axis=1) \n",
    "\n",
    "        self.split2   = pd.DataFrame(self.data[target_cols])\n",
    "\n",
    "        return self.split1, self.split2\n",
    "    \n",
    "    \n",
    "    def encode (self, use_split1=False, use_split2 = False, use_data=False): \n",
    "        \n",
    "        if use_data:\n",
    "      \n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "            to_encode = self.data.select_dtypes(exclude='number')\n",
    "            if self.data.shape[1] > 1:\n",
    "                #ohe = MultiLabelBinarizer()\n",
    "                self.data.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "                features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                self.data = self.data.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "               # print(ohe.classes_) \n",
    "            else: \n",
    "                self.data = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                print(ohe.categories_) \n",
    "\n",
    "            return self.data\n",
    "        \n",
    "        \n",
    "        if use_split1:\n",
    "      \n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "            to_encode = self.split1.select_dtypes(exclude='number')\n",
    "            if self.split1.shape[1] > 1:\n",
    "                #ohe = MultiLabelBinarizer()\n",
    "                self.split1.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "                features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                self.split1 = self.split1.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "               # print(ohe.classes_) \n",
    "            else: \n",
    "                self.split1 = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                print(ohe.categories_) \n",
    "\n",
    "            return self.split1\n",
    "        \n",
    "        \n",
    "        if use_split2:\n",
    "      \n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "            to_encode = self.split2.select_dtypes(exclude='number')\n",
    "            if self.split2.shape[1] > 1:\n",
    "                #ohe = MultiLabelBinarizer()\n",
    "                self.split2.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "                features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                self.split2 = self.split2.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "               # print(ohe.classes_) \n",
    "            else: \n",
    "                self.split2 = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                print(ohe.categories_) \n",
    "\n",
    "            return self.split2\n",
    "            \n",
    "\n",
    "    def scale_data(self, scaler=RobustScaler(),\n",
    "                  use_data=False, use_split1= False, use_split2 = False):\n",
    "        \n",
    "        \"\"\"\n",
    "            Specify scaler type, scaler type must have fit_transform as a method\n",
    "        \"\"\"\n",
    "        \n",
    "        if use_data:\n",
    "            self.data = scaler.fit_transform(self.data)\n",
    "            return self.data \n",
    "        \n",
    "        if use_split1:\n",
    "            self.split1 = scaler.fit_transform(self.split1)\n",
    "            return self.split1\n",
    "        \n",
    "        if use_split2:\n",
    "            self.split2 = scaler.fit_transform(self.split2)\n",
    "            return self.split2\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module uni_plot.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting uni_plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile uni_plot.py\n",
    "#%%writefile ../scripts/project_package/plot_package/uni_plot.py\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import *\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def plot_univariate (data, x=None, y=None, color='r',save=False,\n",
    "                title='New Chart', chart_type='hist', xlabel='', ylabel='',\n",
    "                    save_to=os.getcwd(), log_normalise=False):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Make a univariate plot of any of these selcted types:\n",
    "    \n",
    "    1. bar - barchart\n",
    "    \n",
    "    2. hist - Histogram\n",
    "    \n",
    "    3. pie - Piechart\n",
    "    \n",
    "    4. count - Countplot\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.subplots(figsize=(10,7))\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel(ylabel, fontsize=15)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    \n",
    "    if chart_type == 'hist':\n",
    "        if log_normalise:\n",
    "            data = np.log(data)\n",
    "        plot = sns.distplot(a=data, color=color)\n",
    "        if save:\n",
    "            plt.savefig(fname=save_to+f'/{title}.png', format='png')\n",
    "        \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module bi_plot.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bi_plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bi_plot.py\n",
    "#%%writefile ../scripts/project_package/plot_package/bi_plot.py\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import *\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "\n",
    "def plot_bivariate(data, x=None, y=None, hue=None, \n",
    "                  color='r',save=False,\n",
    "                title='New Chart', chart_type='hist',\n",
    "                   xlabel='', ylabel='',\n",
    "                    save_to=os.getcwd(), img_name = \" \", \n",
    "                   palette={'use':False, \"size\":1}, log_normalise=False,\n",
    "                  kind_joint_plot = 'scatter', kind_pair_plot=\"scatter\", figsize=(10,7)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make a bivariate plot of any of the selcted types:\n",
    "    \n",
    "    1. bar - barchart\n",
    "    \n",
    "    2. scatter  - scatter plot\n",
    "    \n",
    "    3. cat  - catplot\n",
    "    \n",
    "    4. count - countplot\n",
    "    \n",
    "    5 joint - jointplot \n",
    "    \n",
    "    6  pair - pairplot\n",
    "    \n",
    "    7  corr - corr_plot\n",
    "    \n",
    "    When calling joint_plot:\n",
    "        \n",
    "        kind_joint_plot is default to `scatter`\n",
    "        other types include \"reg\", \"reside\", \"kde\", \"hex\"\n",
    "        \n",
    "    When calling pair_plot:\n",
    "        \n",
    "        kind_pair_plot is default to `scatter`\n",
    "        other types include 'reg'\n",
    "    \"\"\"\n",
    "    def plt_tweaks():\n",
    "        plt.subplots(figsize= figsize)\n",
    "        plt.title(title, fontsize=18)\n",
    "        plt.xlabel(xlabel, fontsize=15)\n",
    "        plt.ylabel(ylabel, fontsize=15)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "    \n",
    "    \n",
    "    # define helper functions\n",
    "    \n",
    "    def use_palette():\n",
    "        palettes = []\n",
    "#        palette_to_use=[]\n",
    "        if palette['use'] == True:\n",
    "            palette_to_use = [palettes[i] for i in range(palette['size'])]\n",
    "            \n",
    "            return palette_to_use\n",
    "\n",
    "    def log_norm():\n",
    "        if log_normalise and y != None:\n",
    "            y = np.log(y)\n",
    "        elif log_normalise and y == None:\n",
    "            data = np.log(data)\n",
    "            \n",
    "    def save_image():\n",
    "        if save:\n",
    "            if img_name != \" \":\n",
    "                plt.savefig(fname=save_to+\"/\"+img_name+'.png', format='png')\n",
    "            else:\n",
    "                plt.savefig(fname=save_to+f'/{title}.png', format='png')\n",
    "                \n",
    "        \n",
    "    # make plots\n",
    "    \n",
    "    if chart_type == \"joint\":\n",
    "        log_norm()\n",
    "        plot = sns.jointplot(x=x, y=y, data=data,\n",
    "                            height=6, ratio=5, space=0.2, kind=kind_joint_plot)\n",
    "        \n",
    "        save_image()\n",
    "        \n",
    "    if chart_type == \"pair\":\n",
    "       # try:\n",
    "        log_norm()\n",
    "        if palette['use'] == True:\n",
    "            palette_to_use = use_palette()\n",
    "            plot = sns.pairplot(data, palette=palette_to_use, \n",
    "                            kind= kind_pair_plot,height=3, aspect=1, hue=hue)\n",
    "        else:\n",
    "             plot = sns.pairplot(data, \n",
    "                            kind= kind_pair_plot,height=2.5, aspect=1, hue=hue, )\n",
    "        save_image()\n",
    "        \n",
    "    if chart_type  == \"corr\":\n",
    "        plt_tweaks()\n",
    "        corr_data = data.corr()\n",
    "        corr_plot = sns.heatmap(corr_data,annot=True, fmt='.2g', center=0) \n",
    "        \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module model.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "#%%writefile ../scripts/project_package/model_package/model.py\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import *\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.pipeline import *\n",
    "\n",
    "from imblearn.metrics import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.base import *\n",
    "from sklearn.model_selection import * \n",
    "\n",
    "def plot_pca_components(data):\n",
    "    pca = PCA().fit(data)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance');\n",
    "    \n",
    "def check_imbalance(data,label='', x=0.7, y=30000):\n",
    "    plt.subplots(figsize=(10,8)) \n",
    "    data[label].value_counts().plot(kind='bar')\n",
    "    text = f'Class Imbalance Count:\\n\\n{data[label].value_counts().to_dict()}'\n",
    "    plt.text(x=x, y=y, s = text ,  fontsize=15)\n",
    "    \n",
    "def encode (data):\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "    to_encode = data.select_dtypes(exclude='number')\n",
    "    if data.shape[1] > 1:\n",
    "        #ohe = MultiLabelBinarizer()\n",
    "        data.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "        features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "        data = data.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "        #print(ohe.classes_) \n",
    "    else:\n",
    "        data = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "        print(ohe.categories_) \n",
    "    return data \n",
    "\n",
    "\n",
    "def x_y_split(data, x=None, y=None, type_=\"single\", test_size=.10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Single type divides into just x and y\n",
    "    Double type divides into train and test for each of x and y\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = data.drop(columns=y, axis=1), data[y]\n",
    "    \n",
    "    if type_ == \"single\":\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    if type == \"double\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                               test_size=test_size, random_state=123)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "def gridSearch(model,hyper_params={},cv=StratifiedKFold(), x_train=None, y_train=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs GridSeach of the best hyperparmaters for the passed model\n",
    "    \"\"\"\n",
    "    \n",
    "    search = GridSearchCV(model=model, param_grid = hyper_params, n_jobs=-1, cv=cv)\n",
    "    search.fit(X=x_train, y=y_train)\n",
    "    print(\"Best parameter (CV score=%0.3f):\\n\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "    print(search.score) \n",
    "    return search\n",
    "\n",
    "\n",
    "def plot_grid_search(search_obj, pca_obj, X_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints the best (optimised) hyperparmatersfor the grid search object\n",
    "    and plots the optimised pca components\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Best parameter (CV score=%0.3f):\\n\" % search.best_score_)\n",
    "    print(\"Best Params:\",search.best_params_)\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(8, 8))\n",
    "    ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "             pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "    ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "    ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "                linestyle=':', label='n_components chosen')\n",
    "    ax0.legend(prop=dict(size=12))\n",
    "\n",
    "    # For each number of components, find the best classifier results\n",
    "    results = pd.DataFrame(search.cv_results_)\n",
    "    components_col = 'param_pca__n_components'\n",
    "    best_clfs = results.groupby(components_col).apply(\n",
    "        lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "    best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "                   legend=False, ax=ax1)\n",
    "    ax1.set_ylabel('Classification accuracy (val)')\n",
    "    ax1.set_xlabel('n_components')\n",
    "\n",
    "    plt.xlim(-1, 70)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return \"Used to prepare data for modelling\"\n",
    "    \n",
    "    def  __init__(self):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        \n",
    "        assert(type(data) is pandas.core.frame.DataFrame), \"data must be of type pandas.DataFrame\"\n",
    "        \n",
    "        self.data = data \n",
    "        \n",
    "        print(\"Fitted\")\n",
    "        \n",
    "        return self \n",
    "        \n",
    "\n",
    "\n",
    "    def check_outliers(self, show_plot=False, save_img=os.getcwd()+'/outliers.png'):\n",
    "            \n",
    "        \"\"\"\n",
    "        This functions checks for columns with outlers using the IQR method\n",
    "\n",
    "        It accespts as argmuent a dataset. \n",
    "        show_plot can be set to True to output pairplots of outlier columns    \n",
    "        \"\"\"\n",
    "\n",
    "        self.outliers = [] \n",
    "        Q1 = self.data.quantile(0.25)  \n",
    "        Q3 = self.data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        num_data = self.data.select_dtypes(include='number')\n",
    "        result = dict ((((num_data < (Q1 - 1.5 * IQR)) | (num_data > (Q3 + 1.5 * IQR)))==True).any())\n",
    "        #data[(data[col] >= high)|(data[col] <= low)].index\n",
    "        index = self.data[(num_data < Q1 - 1.5 * IQR) | (num_data > Q3 + 1.5 * IQR)].index\n",
    "        for k,v in result.items():\n",
    "            if v == True:  \n",
    "                self.outliers.append(k)\n",
    "        if show_plot:\n",
    "            self.outlier_pair_plot = sns.pairplot(self.data[self.outliers]);\n",
    "            print(f'{result},\\n\\n Visualization of outlier columns')\n",
    "            plt.savefig(fname=save_img, format='png')\n",
    "            return  self.outlier_pair_plot\n",
    "        else:\n",
    "            return self.data.loc[index, self.outliers] \n",
    "        \n",
    "        \n",
    "    def treat_outliers(self, type_='median_replace'):\n",
    "            \n",
    "        \"\"\"\n",
    "        This treat outliers using any ofthses 3 methods as specified by user\n",
    "\n",
    "            1. median_replace -  median replacement\n",
    "\n",
    "            2. quant_floor - quantile flooring\n",
    "\n",
    "            3. trim - trimming \n",
    "\n",
    "            4. log_transform - log transformations\n",
    "\n",
    "        The methods are some of the commont statistical methods in treating outler\n",
    "        columns\n",
    "\n",
    "        By default treatment type is set to median replacement\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if type_ == \"median_replace\":\n",
    "\n",
    "            for col in self.data.columns.tolist():\n",
    "                if is_numeric_dtype(self.data[col]):\n",
    "                    median = (self.data[col].quantile(0.50))\n",
    "                    q1 = self.data[col].quantile(0.25)\n",
    "                    q3 = self.data[col].quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    high = int(q3 + 1.5 * iqr) \n",
    "                    low = int(q1 - 1.5 * iqr)\n",
    "                    self.data[col] = np.where(self.data[col] > high, median, self.data[col])\n",
    "                    self.data[col] = np.where(self.data[col] > high, median, self.data[col])        \n",
    "\n",
    "        if type_ == \"quant_floor\":\n",
    "\n",
    "            for col in self.data.columns.tolist():\n",
    "                if is_numeric_dtype(data[col]):\n",
    "                    q_10 = self.data[col].quantile(0.5)\n",
    "                    q_90 = self.data[col].quantile(0.95)\n",
    "                    self.data[col] =  self.data[col] = np.where(self.data[col] < q_10, q_10 , self.data[col])\n",
    "                    self.data[col] =  self.data[col] = np.where(self.data[col] > q_90, q_90 , self.data[col])\n",
    "\n",
    "        if type_ == \"trim\": \n",
    "\n",
    "            for col in self.data.columns.tolist():\n",
    "                low = .05\n",
    "                high = .95\n",
    "                quant_df = self.data.quantile([low, high])\n",
    "                for name in list(self.data.columns):\n",
    "                    if is_numeric_dtype(self.data[name]):\n",
    "                        self.data = self.data[(self.data[name] >= quant_df.loc[low, name]) \n",
    "                            & (self.data[name] <= quant_df.loc[high, name])]\n",
    "\n",
    "        if type_ == \"log_transform\":  \n",
    "            for col in self.data.columns.tolist():\n",
    "                if is_numeric_dtype(self.data[col]):\n",
    "                    self.data[col] = self.data[col].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "\n",
    "        if type_ == \"isf\":\n",
    "            iso = IsolationForest(contamination=0.1)\n",
    "            yhat = iso.fit_predict(self.data.select_dtypes(exclude='object'))\n",
    "            #select all rows that are not outliers\n",
    "            mask = yhat != -1 \n",
    "            self.data = self.data[mask]\n",
    "\n",
    "\n",
    "        return self.data \n",
    "    \n",
    "    \n",
    "    def map_col_values(self, col_name=\"\", values_dict={}):\n",
    "\n",
    "        self.data[col_name] = self.data[col_name].map(values_dict)\n",
    "\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    def split_data_single(self, target_cols=[]):\n",
    "            \n",
    "        self.features = self.data.drop(columns=target_cols, axis=1) \n",
    "\n",
    "        self.target   = pd.DataFrame(self.data[target_cols])\n",
    "\n",
    "        return self.features, self.target\n",
    "    \n",
    "    \n",
    "    def encode (self, data_obj=None, use_features=True, use_target=False): \n",
    "        \n",
    "        if data_obj is None and use_features == False and use_target == False:\n",
    "        \n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "            to_encode = self.data.select_dtypes(exclude='number')\n",
    "            if self.data.shape[1] > 1:\n",
    "                #ohe = MultiLabelBinarizer()\n",
    "                self.data.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "                features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                self.data = self.data.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "               # print(ohe.classes_) \n",
    "            else: \n",
    "                self.data = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                print(ohe.categories_) \n",
    "            return self.data\n",
    "        \n",
    "        if data_obj is not None:\n",
    "        \n",
    "            self.data_obj = data_obj\n",
    "            print(\"Not None\")\n",
    "            \n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "            to_encode = self.data_obj.select_dtypes(exclude='number')\n",
    "            if self.data_obj.shape[1] > 1:\n",
    "                #ohe = MultiLabelBinarizer()\n",
    "                self.data_obj.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "                features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                self.data_obj = self.data_obj.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "               # print(ohe.classes_) \n",
    "            else:\n",
    "                self.data_obj = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                print(ohe.categories_) \n",
    "            return self.data_obj\n",
    "        \n",
    "        if use_features:\n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "            to_encode = self.features.select_dtypes(exclude='number')\n",
    "            if self.features.shape[1] > 1:\n",
    "                #ohe = MultiLabelBinarizer()\n",
    "                self.features.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "                features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                self.features = self.features.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "               # print(ohe.classes_) \n",
    "            else: \n",
    "                self.features = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                print(ohe.categories_) \n",
    "            return self.features\n",
    "        \n",
    "        if use_target:\n",
    "            \n",
    "            ohe = OneHotEncoder(sparse=False, handle_unknown='ignore', )\n",
    "            to_encode = self.target.select_dtypes(exclude='number')\n",
    "            if self.target.shape[1] > 1:\n",
    "                #ohe = MultiLabelBinarizer()\n",
    "                self.target.drop(to_encode.columns.tolist(), axis=1, inplace = True)\n",
    "                features_cat_encode = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                self.target = self.target.merge(features_cat_encode, left_index=True, right_index=True)\n",
    "               # print(ohe.classes_) \n",
    "            else: \n",
    "                self.target = pd.DataFrame(ohe.fit_transform(to_encode))\n",
    "                print(ohe.categories_) \n",
    "            return self.target\n",
    "            \n",
    "    \n",
    "    def split_data_double(self, features_=pd.DataFrame([[]]), target_=pd.DataFrame([[]]), \n",
    "                          test_size=.10, use_native=True):\n",
    "        \n",
    "        if use_native == False:\n",
    "        \n",
    "            if features.shape[0] != target.shape[0]:\n",
    "                \n",
    "                raise Exception(\"Wrong, you are trying to pass unequal shapes\\n\\\n",
    "                Shapes of dataframes must be equal\\n\\\n",
    "                Try target = target.iloc[0:features.shape[0]]\")\n",
    "\n",
    "            self.features_ = features_\n",
    "            self.target_ = target_\n",
    "\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.features_, \n",
    "                                                                                    self.target_,\n",
    "                                           test_size= test_size, random_state=24)\n",
    "\n",
    "            return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "        \n",
    "        if use_native:\n",
    "            \n",
    "            self.target = self.target.iloc[0:self.features.shape[0]]\n",
    "            \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.features, self.target,\n",
    "                                           test_size= test_size, random_state=24)\n",
    "\n",
    "            return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def scale_data(self, scale_data=pd.DataFrame([[]]),\n",
    "                   scaler=RobustScaler(), use_features=True,\n",
    "                  use_target=False, use_data=False):\n",
    "        \n",
    "        \"\"\"\n",
    "            Specify scaler type, scaler type must have fit_transform as a method\n",
    "        \"\"\"\n",
    "        \n",
    "        if use_features:\n",
    "    \n",
    "            self.features = scaler.fit_transform(self.features)\n",
    "\n",
    "            return self.features\n",
    "        \n",
    "        if use_target:\n",
    "            \n",
    "            self.target = scaler.fit_transform(self.target)\n",
    "\n",
    "            return self.target\n",
    "        \n",
    "        if use_data:\n",
    "            \n",
    "            self.data = scaler.fit_transform(self.data)\n",
    "\n",
    "            return self.data\n",
    "        \n",
    "        if use_data == False and use_features == False and use_target == False:\n",
    "            \n",
    "            self.scale_data = scale_data\n",
    "            \n",
    "            self.scale_data = scaler.fit_transform(self.scale_data)\n",
    "\n",
    "            return self.scale_data\n",
    "            \n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Ideally, a preapred trainX data ought to be passed to in case of passing into a pipeline\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = X\n",
    "                \n",
    "        self.data = self.treat_outliers(type_=\"isf\") \n",
    "        \n",
    "        #self.data = self.map_col_values(col_name=\"y\", values_dict={\"no\":0, \"yes\":1})\n",
    "        \n",
    "       # self.features, self.target = self.split_data_single(target_cols=[\"y\"])\n",
    "        #print(self.features)\n",
    "                \n",
    "        self.features = self.encode(self.features)\n",
    "      #  self.target = self.target.iloc[0:self.features.shape[0], 0:]\n",
    "        #print(self.target)\n",
    "       # self.X_train, self.X_test, self.y_train, self.y_test = self.split_data_double(\n",
    "        #    self.features, self.target, test_size=.10)\n",
    "        \n",
    "        scaler=RobustScaler() \n",
    "            \n",
    "        X = scaler.fit_transform(self.X_train)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        return self.transform(self.X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module model_metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_metrics.py\n",
    "#%%writefile ../scripts/project_package/model_package/model_metrics.py\n",
    "\n",
    "from imblearn.metrics import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.base import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class Metrics ():\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_test, y_test, y_hat=np.array([])):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.y_hat =  y_hat\n",
    "        \n",
    "    \n",
    "    def class_report(self, y_hat):\n",
    "        \n",
    "        self.y_hat = y_hat\n",
    "        \n",
    "        full_report = classification_report_imbalanced(self.y_test, self.y_hat)\n",
    "        \n",
    "        print(full_report)\n",
    "        \n",
    "    def conf_matrix(self, y_hat):\n",
    "        self.y_hat = y_hat\n",
    "        \n",
    "        conf_matrix = confusion_matrix(self.y_test, self.y_hat)\n",
    "        \n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Actual_+ve', 'Actual_-ve'],\n",
    "                               index=['predicted_+ve', 'predicted_-ve'])\n",
    "        \n",
    "        return conf_matrix_df\n",
    "    \n",
    "    def balanced_accuracy_score(self, y_hat):\n",
    "        \n",
    "        self.y_hat = y_hat\n",
    "        return  balanced_accuracy_score(self.y_test, self.y_hat)\n",
    "    \n",
    "    def balanced_classification_error(self, y_hat):\n",
    "        self.y_hat = y_hat\n",
    "        return 1 - balanced_accuracy_score(self.y_test, self.y_hat) \n",
    "        \n",
    "    def specif_sensitiv(self, y_hat):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Sensitivity: When the actual value is positive, how often is the prediction correct?\n",
    "        \n",
    "        Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "        \"\"\"\n",
    "        \n",
    "        self.y_hat = y_hat\n",
    "        conf_matrix = confusion_matrix(self.y_test, self.y_hat)\n",
    "        \n",
    "        #vals = self.y_test, self.y_hat\n",
    "        \n",
    "        TP = conf_matrix[1, 1]\n",
    "        TN = conf_matrix[0, 0]\n",
    "        FP = conf_matrix[0, 1]\n",
    "        FN = conf_matrix[1, 0]\n",
    "        \n",
    "        sensitiv = TP / float(FN + TP)\n",
    "        specific = TN / (TN + FP)\n",
    "        \n",
    "        # this functions was updated to use the provided speciictiy/sensitivity score provided by Imblearn.metrics\n",
    "        \n",
    "        # but the manual calculation was retained for record purposes\n",
    "        \n",
    "        sensitiv_specific_table = pd.DataFrame([[sensitivity_score(self.y_test, self.y_hat),\n",
    "                                                 specificity_score(self.y_test, self.y_hat)]],\n",
    "                                               columns=['sensitivity', 'specificity'])\n",
    "        \n",
    "        return sensitiv_specific_table\n",
    "    \n",
    "    \n",
    "    def evaluate_classifier(self, clf, models_eval_scores, clf_name=None): # X_train, y_train, X_test, y_test, ):\n",
    "    \n",
    "        self.clf = clf\n",
    "        self.models_eval_scores = models_eval_scores\n",
    "        self.clf_name = clf_name\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns a dataframe of unbalanced and balanced acuuracy score of estimators used\n",
    "        Run for the first time, pass an empty dataframe of df_scorees \n",
    "        and when running on more estimators, pass the previous df_scores dataframe for a \n",
    "        single table of evaluation scores\n",
    "\n",
    "        Example given below:\n",
    "\n",
    "                        LogisticRegressionCV\n",
    "                Accuracy \t        0.908\n",
    "                Balanced accuracy \t0.842\n",
    "\n",
    "        \"\"\"\n",
    "        from imblearn.pipeline import Pipeline as ImbPipe\n",
    "        from sklearn.pipeline import Pipeline as Pipe\n",
    "        if self.clf_name is None:\n",
    "            if isinstance(self.clf, ImbPipe) or isinstance(self.clf, Pipe):\n",
    "                self.clf_name = self.clf[-1].__class__.__name__\n",
    "                print(self.clf_name)\n",
    "            else:\n",
    "                self.clf_name = self.clf.__class__.__name__\n",
    "                print(self.clf_name)\n",
    "        acc = self.clf.fit(self.X_train, self.y_train).score(self.X_test, self.y_test)\n",
    "        y_pred = self.clf.predict(self.X_test)\n",
    "        bal_acc = balanced_accuracy_score(self.y_test, y_pred)\n",
    "        print(f\"acc->{acc}, bal_acc->{bal_acc}\")\n",
    "        clf_score = pd.DataFrame(\n",
    "            {self.clf_name: [acc, bal_acc]}, \n",
    "            index=['Accuracy', 'Balanced accuracy']\n",
    "        )\n",
    "        self.models_eval_scores = pd.concat([self.models_eval_scores, clf_score], axis=1).round(decimals=3)\n",
    "        \n",
    "        return self.models_eval_scores\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_hat,ncols=1,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "        \n",
    "        self.y_hat = y_hat \n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, self.y_hat)\n",
    "        \n",
    "        classes = self.y_train\n",
    "        \n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "       \n",
    "        \"\"\" \n",
    "        print(cm)\n",
    "        print('')\n",
    "\n",
    "        ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.sca(ax)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd' \n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        ax.set_ylabel('True label')\n",
    "        ax.set_xlabel('Predicted label')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def plot_roc(self, y_hat, n_classes=2):\n",
    "        \n",
    "        self.y_hat = y_hat\n",
    "        \n",
    "       # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(self.y_test[:, i],  self.y_hat[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(self.y_test, self.y_hat)\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        \n",
    "\n",
    "                # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Plot all ROC curves\n",
    "        plt.figure()\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                     label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                     ''.format(i, roc_auc[i]))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_selection.from_model module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#%%writefile main.py\n",
    "#%%writefile ../scripts/project_package/main.py\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from pandas.api.types import is_numeric_dtype \n",
    "\n",
    "from plot import plot_univariate, plot_bivariate\n",
    "from data import WrangleData\n",
    "from model import check_imbalance, plot_pca_components, encode, x_y_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE, _random_over_sampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_classif, from_model, SelectKBest,chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest # outlier detection and re,oval\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from imblearn.pipeline import Pipeline as ImbPipe\n",
    "import joblib\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module pipeline.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline.py\n",
    "#%%writefile ../scripts/project_package/model_package/pipeline.py \n",
    "from sklearn.pipeline import FeatureUnion, Pipeline  \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_termsheet_pred_model",
   "language": "python",
   "name": "venv_termsheet_pred_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
